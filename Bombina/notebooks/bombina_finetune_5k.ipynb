{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f28740",
   "metadata": {},
   "source": [
    "# üê∏ Bombina Fine-Tuning - 5K Dataset\n",
    "\n",
    "**Professional Pentest AI Fine-Tuning with Unsloth + QLoRA**\n",
    "\n",
    "Dataset: 5,004 reasoning-based samples\n",
    "- Attack decision paths\n",
    "- Failure analysis\n",
    "- Detection evasion\n",
    "- Blue team perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272b398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install unsloth\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5422f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "max_seq_length = 4096\n",
    "dtype = None  # Auto-detect\n",
    "load_in_4bit = True\n",
    "\n",
    "# Load base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen2.5-7B\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model.config._name_or_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb69ff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA configuration (optimized for pentest reasoning)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA applied\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2acf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your dataset\n",
    "from google.colab import files\n",
    "print(\"üìÅ Upload train_split.jsonl from scripts/data/processed/\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b578e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and format dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Alpaca-style prompt template for pentest reasoning\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a penetration testing task, paired with an input that provides further context. Write a response that appropriately completes the request with expert-level reasoning.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        text = alpaca_prompt.format(\n",
    "            instruction=instruction,\n",
    "            input=input_text if input_text else \"\",\n",
    "            output=output\n",
    "        ) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"train_split.jsonl\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"\\nüìù Sample entry:\\n{dataset[0]['text'][:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852b3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_ratio=0.05,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=42,\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configured\")\n",
    "print(f\"   Batch size: 2 x 8 = 16 effective\")\n",
    "print(f\"   Epochs: 3\")\n",
    "print(f\"   Learning rate: 2e-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4fc180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ START TRAINING\n",
    "print(\"üî• Starting fine-tuning...\")\n",
    "print(\"   Expected time: ~2-3 hours on T4/A100\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(f\"   Total steps: {trainer_stats.global_step}\")\n",
    "print(f\"   Final loss: {trainer_stats.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c20fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_prompt = alpaca_prompt.format(\n",
    "    instruction=\"You are conducting a penetration test against a Windows domain.\",\n",
    "    input=\"You have local admin on a workstation. EDR is deployed. What's your next move?\",\n",
    "    output=\"\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "print(\"üß™ Test Response:\")\n",
    "print(response.split(\"### Response:\")[1] if \"### Response:\" in response else response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a2d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter\n",
    "model.save_pretrained(\"bombina-lora-5k\")\n",
    "tokenizer.save_pretrained(\"bombina-lora-5k\")\n",
    "print(\"‚úÖ LoRA adapter saved to bombina-lora-5k/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4105d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to GGUF for Ollama\n",
    "print(\"üì¶ Exporting to GGUF format...\")\n",
    "\n",
    "model.save_pretrained_gguf(\n",
    "    \"bombina-5k\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\"  # Good balance of size/quality\n",
    ")\n",
    "\n",
    "print(\"‚úÖ GGUF exported: bombina-5k-q4_k_m.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Find the GGUF file\n",
    "for f in os.listdir(\"bombina-5k\"):\n",
    "    if f.endswith(\".gguf\"):\n",
    "        print(f\"üì• Downloading {f}...\")\n",
    "        files.download(f\"bombina-5k/{f}\")\n",
    "        break\n",
    "\n",
    "print(\"\\nüìã To use with Ollama:\")\n",
    "print(\"1. Copy .gguf to your machine\")\n",
    "print(\"2. Create Modelfile:\")\n",
    "print('   FROM ./bombina-5k-q4_k_m.gguf')\n",
    "print('   PARAMETER num_ctx 4096')\n",
    "print('   PARAMETER temperature 0.7')\n",
    "print(\"3. ollama create bombina-5k -f Modelfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85feec02",
   "metadata": {},
   "source": [
    "## üéØ Training Summary\n",
    "\n",
    "**What was trained:**\n",
    "- 5,004 pentest reasoning samples\n",
    "- Attack decision trees\n",
    "- Failure scenarios\n",
    "- Detection awareness\n",
    "- Blue team perspective\n",
    "\n",
    "**Model outputs:**\n",
    "- `bombina-lora-5k/` - LoRA adapter (~300MB)\n",
    "- `bombina-5k-q4_k_m.gguf` - Quantized for Ollama (~4GB)\n",
    "\n",
    "**Next steps:**\n",
    "1. Download GGUF\n",
    "2. Create Ollama model\n",
    "3. Test with Bombina agent"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
